{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091095d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Project     : Chameleon - Make KO-EN translator\n",
    "# Created By  : Eungi\n",
    "# Team        : Product Growth - Data Science\n",
    "# Created Date: 2023-07-06\n",
    "# Updated Date: 2023-07-07\n",
    "# Purpose     : Make data_loader for loading corpus data\n",
    "# version     : 0.0.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e2127a-b18c-4491-b501-10d64d9668bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ToolCadeau.preprocessors.refiner import refine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dc9ba3f",
   "metadata": {},
   "source": [
    "# Train - Valid - Test\n",
    "- make src and tgt column\n",
    "- tokenize `src` using `Sentencepiece`, and tokenize adjustly of `tgt`\n",
    "- create train, valid, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87674489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, delimiter=\"\\t\"):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        kos, ens = [], []\n",
    "        for line in file:\n",
    "            ko, en = line.strip().split(delimiter)\n",
    "            kos += [ko]\n",
    "            ens += [en]\n",
    "\n",
    "    data = pd.DataFrame({\"ko\": kos, \"en\": ens})\n",
    "    return data\n",
    "\n",
    "\n",
    "DATA_ROOT = \"../data/\"\n",
    "data = read_data(DATA_ROOT + \"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64880059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:10<00:00, 65.18s/it]\n"
     ]
    }
   ],
   "source": [
    "def refine_data(data, regex_fn):\n",
    "    cols = data.columns.unique().tolist()\n",
    "\n",
    "    for col in tqdm(cols):\n",
    "        data[col] = refine(data, column=col, regex_fn=regex_fn)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "REGEX_FN = \"./refine.regex.txt\"\n",
    "data = refine_data(data, REGEX_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6157e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(DATA_ROOT + \"corpus.refine.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc980fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1281934 160242 160242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_valid_test(data):\n",
    "    train_data, test_data = train_test_split(\n",
    "        data,\n",
    "        test_size=0.2,\n",
    "        random_state=1004,\n",
    "    )\n",
    "\n",
    "    test_data, valid_data = train_test_split(\n",
    "        test_data, test_size=0.5, random_state=1004\n",
    "    )\n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    valid_data = valid_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    print(len(train_data), len(valid_data), len(test_data))\n",
    "\n",
    "    train_data.to_pickle(DATA_ROOT + \"chameleon.train.pickle\")\n",
    "    valid_data.to_pickle(DATA_ROOT + \"chameleon.valid.pickle\")\n",
    "    test_data.to_pickle(DATA_ROOT + \"chameleon.test.pickle\")\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = split_train_valid_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30032114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_srcs(train_data):\n",
    "    cols = train_data.columns.unique().tolist()\n",
    "    for col in cols:\n",
    "        fn = f\"chameleon.train.{col}.txt\"\n",
    "        srcs = train_data[col].unique().tolist()\n",
    "        with open(DATA_ROOT + fn, \"w\") as file:\n",
    "            for src in srcs:\n",
    "                if src.strip() != \"\":\n",
    "                    file.write(src.strip() + \"\\n\")\n",
    "                else:\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "save_train_srcs(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6e9dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/chameleon.train.ko.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/ko.train.bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/chameleon.train.ko.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1280087), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1280087 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=74976744\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=1366\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1280087 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=28094457\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 1001366 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1280087\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 1823256\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 1823256 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=563463 obj=14.63 num_tokens=4252220 num_tokens/piece=7.54658\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=490719 obj=13.3504 num_tokens=4268622 num_tokens/piece=8.69871\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=367991 obj=13.3035 num_tokens=4395597 num_tokens/piece=11.9448\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=367683 obj=13.2744 num_tokens=4397781 num_tokens/piece=11.9608\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=275761 obj=13.3459 num_tokens=4561149 num_tokens/piece=16.5402\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=275757 obj=13.3276 num_tokens=4561551 num_tokens/piece=16.5419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=206817 obj=13.4233 num_tokens=4731930 num_tokens/piece=22.8798\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=206817 obj=13.4025 num_tokens=4731770 num_tokens/piece=22.879\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=155112 obj=13.524 num_tokens=4909639 num_tokens/piece=31.6522\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=155112 obj=13.4997 num_tokens=4909513 num_tokens/piece=31.6514\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=116334 obj=13.6461 num_tokens=5094026 num_tokens/piece=43.7879\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=116334 obj=13.617 num_tokens=5094064 num_tokens/piece=43.7883\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=87250 obj=13.7908 num_tokens=5287241 num_tokens/piece=60.5988\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=87250 obj=13.7552 num_tokens=5287193 num_tokens/piece=60.5982\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65437 obj=13.9604 num_tokens=5486910 num_tokens/piece=83.8503\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65437 obj=13.9177 num_tokens=5487156 num_tokens/piece=83.854\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=55000 obj=14.0527 num_tokens=5607254 num_tokens/piece=101.95\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=55000 obj=14.0248 num_tokens=5607251 num_tokens/piece=101.95\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../data/ko.train.bpe.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../data/ko.train.bpe.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/chameleon.train.en.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/en.train.bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/chameleon.train.en.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1279517), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1279517 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=182585616\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9615% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=70\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999615\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1279517 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=95429967\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 436377 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1279517\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 522785\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 522785 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=176847 obj=11.0701 num_tokens=1265517 num_tokens/piece=7.156\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=149922 obj=8.49976 num_tokens=1268352 num_tokens/piece=8.46008\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=112427 obj=8.46055 num_tokens=1306644 num_tokens/piece=11.6222\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=112342 obj=8.4572 num_tokens=1307656 num_tokens/piece=11.64\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=84255 obj=8.47155 num_tokens=1364656 num_tokens/piece=16.1967\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=84253 obj=8.46951 num_tokens=1364575 num_tokens/piece=16.1962\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=63189 obj=8.49104 num_tokens=1430326 num_tokens/piece=22.6357\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=63188 obj=8.48658 num_tokens=1430155 num_tokens/piece=22.6333\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=47391 obj=8.51884 num_tokens=1501435 num_tokens/piece=31.6819\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=47391 obj=8.51296 num_tokens=1501389 num_tokens/piece=31.6809\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=35543 obj=8.56176 num_tokens=1579454 num_tokens/piece=44.4378\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=35543 obj=8.55348 num_tokens=1579219 num_tokens/piece=44.4312\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33000 obj=8.568 num_tokens=1598762 num_tokens/piece=48.4473\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33000 obj=8.56566 num_tokens=1598836 num_tokens/piece=48.4496\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../data/en.train.bpe.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../data/en.train.bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def train_spm(cols) -> None:\n",
    "    vocab_size = {\"ko\": 50000, \"en\": 30000}\n",
    "\n",
    "    for col in cols:\n",
    "        fn = f\"chameleon.train.{col}.txt\"\n",
    "        input_params = {\n",
    "            \"input\": DATA_ROOT + fn,\n",
    "            \"model_prefix\": DATA_ROOT + f\"{col}.train.bpe\",\n",
    "            \"vocab_size\": vocab_size[col],\n",
    "        }\n",
    "        spm.SentencePieceTrainer.train(**input_params)\n",
    "\n",
    "\n",
    "cols = data.columns.unique().tolist()\n",
    "# ['ko', 'en']\n",
    "train_spm(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e05fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_train_valid_test_src(train_data, valid_data, test_data):\n",
    "    cols = train_data.columns.unique().tolist()\n",
    "    types = [\"train\", \"valid\", \"test\"]\n",
    "    # ['ko', 'en']\n",
    "\n",
    "    for col in cols:\n",
    "        sp = spm.SentencePieceProcessor(model_file=DATA_ROOT + f\"{col}.train.bpe.model\")\n",
    "\n",
    "        for type in types:\n",
    "            tgt_data = locals()[type + \"_\" + \"data\"]\n",
    "            srcs = tgt_data[col].tolist()\n",
    "            encoded_srcs = sp.encode(srcs, out_type=str)\n",
    "            tgt_data[f\"tok_{col}\"] = [\n",
    "                \" \".join(encoded_src) for encoded_src in encoded_srcs\n",
    "            ]\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = tokenize_train_valid_test_src(\n",
    "    train_data, valid_data, test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301872f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle(DATA_ROOT + \"chameleon.train.tok.pickle\")\n",
    "valid_data.to_pickle(DATA_ROOT + \"chameleon.valid.tok.pickle\")\n",
    "test_data.to_pickle(DATA_ROOT + \"chameleon.test.tok.pickle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e833a194",
   "metadata": {},
   "source": [
    "## Make src and tgt\n",
    "- use Vocabulary, TranslationDataset, DataLoader to make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a14281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    # pre-defined token idx\n",
    "    PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_freq=1,\n",
    "        max_vocab=99999,\n",
    "    ):\n",
    "        # Default Vocabulary\n",
    "        self.itos = {\n",
    "            Vocabulary.PAD: \"<PAD>\",\n",
    "            Vocabulary.BOS: \"<BOS>\",\n",
    "            Vocabulary.EOS: \"<EOS>\",\n",
    "            Vocabulary.UNK: \"<UNK>\",\n",
    "        }\n",
    "        self.stoi = {token: idx for idx, token in self.itos.items()}\n",
    "\n",
    "        self.min_freq = min_freq\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text, delimiter):\n",
    "        return [tok.strip() for tok in text.split(delimiter)]\n",
    "\n",
    "    def build_vocab(self, sents, delimiter):\n",
    "        # bag of words\n",
    "        bow = defaultdict(int)\n",
    "\n",
    "        for sent in sents:\n",
    "            words = self.tokenizer(sent, delimiter=delimiter)\n",
    "            for word in words:\n",
    "                bow[word] += 1\n",
    "\n",
    "        # limit vocab by removing low frequence word\n",
    "        bow = {word: freq for word, freq in bow.items() if freq >= self.min_freq}\n",
    "        bow = sorted(bow.items(), key=lambda x: -x[1])\n",
    "\n",
    "        # limit size of the vocab\n",
    "        bow = dict(bow[: self.max_vocab - len(self.itos)])\n",
    "\n",
    "        # create vocab\n",
    "        start_idx = len(self.itos)\n",
    "\n",
    "        for word in bow.keys():\n",
    "            self.stoi[word] = start_idx\n",
    "            self.itos[start_idx] = word\n",
    "            start_idx += 1\n",
    "\n",
    "        print(\"Number of vocabularies: \", len(self))\n",
    "\n",
    "    def encode(self, text, delimiter):\n",
    "        \"\"\"\n",
    "        Encode text input. Support batch input.\n",
    "        Return list.\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_text = []\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            # |text| = [text1, text2, ...]\n",
    "            tokenized_text = list(map(self.tokenizer, text, repeat(delimiter)))\n",
    "            # |tokenized_text| = [[token1, token2, ...], [token1, token2, ...]]\n",
    "            for tokens in tokenized_text:\n",
    "                encoded_text += [\n",
    "                    [\n",
    "                        self.stoi[token]\n",
    "                        if token in self.stoi.keys()\n",
    "                        else self.stoi[\"<UNK>\"]\n",
    "                        for token in tokens\n",
    "                    ]\n",
    "                ]\n",
    "                # |encoded_text| = [[token_idx1, token_idx2], [token_idx1, token_idx2]]\n",
    "        else:\n",
    "            # |text| = str\n",
    "            tokenized_text = self.tokenizer(text, delimiter=delimiter)\n",
    "            # |tokenized_text| = [token1, token2, ...]\n",
    "            encoded_text += [\n",
    "                self.stoi[token] if token in self.stoi.keys() else self.stoi[\"<UNK>\"]\n",
    "                for token in tokenized_text\n",
    "            ]\n",
    "            # |encoded_text| = [token_idx1, token_idx2, ...]\n",
    "\n",
    "        return encoded_text\n",
    "\n",
    "    def decode(self, indice, delimiter, removed_indice=[BOS, EOS, PAD]):\n",
    "        \"\"\"\n",
    "        Decode indice input. Support batch input.\n",
    "        Return list.\n",
    "        \"\"\"\n",
    "\n",
    "        decoded_indice = []\n",
    "\n",
    "        # check if indice is batch input\n",
    "        if isinstance(indice, torch.Tensor):\n",
    "            is_nested = indice.ndim > 1\n",
    "            indice = indice.tolist()\n",
    "        else:\n",
    "            is_nested = any(isinstance(elm, list) for elm in indice)\n",
    "\n",
    "        if is_nested:\n",
    "            # |indice| = (batch_size, length)\n",
    "            # |indice| = [[idx1, idx2, ...], [idx1, idx2, ...]]\n",
    "            for encoded_text in indice:\n",
    "                decoded = []\n",
    "                for idx in encoded_text:\n",
    "                    if idx in self.itos.keys() and idx not in removed_indice:\n",
    "                        decoded += [self.itos[idx]]\n",
    "                    elif idx in removed_indice:\n",
    "                        continue\n",
    "                    else:\n",
    "                        decoded += [self.itos[Vocabulary.UNK]]\n",
    "\n",
    "                decoded_indice += [delimiter.join(decoded).strip()]\n",
    "\n",
    "        else:\n",
    "            # |indice| = (length, )\n",
    "            # |indice| = [idx1, idx2, ...]\n",
    "            decoded = []\n",
    "            for idx in indice:\n",
    "                if idx in self.itos.keys() and idx not in removed_indice:\n",
    "                    decoded += [self.itos[idx]]\n",
    "                elif idx in removed_indice:\n",
    "                    continue\n",
    "                else:\n",
    "                    decoded += [self.itos[Vocabulary.UNK]]\n",
    "\n",
    "            decoded_indice += [delimiter.join(decoded).strip()]\n",
    "\n",
    "        return decoded_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78a46403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies:  53430\n",
      "Number of vocabularies:  30488\n"
     ]
    }
   ],
   "source": [
    "src_vocab = Vocabulary()\n",
    "src_vocab.build_vocab(train_data[\"tok_ko\"], delimiter=\" \")\n",
    "\n",
    "tgt_vocab = Vocabulary()\n",
    "tgt_vocab.build_vocab(train_data[\"tok_en\"], delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d50e9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from types import NoneType\n",
    "from typing import Type, Union, List\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        srcs (list): Sources to be used as the input data.\n",
    "            Note. Sources must be tokenized before putting into Dataset.\n",
    "        tgts (list): Targets to be used as the target data.\n",
    "            Note. Targets must be tokenized before putting into Dataset.\n",
    "        min_freq (int): Minimum frequency to be included in the vocabulary. Defaults to 1.\n",
    "        max_vocab (int): Maximum size of vocabulary. Defaults to 99999.\n",
    "        src_delimiter (str): Delimiter to tokenize the srcs and tgts.\n",
    "        src_vocab (Vocabulary): Vocabulary to encode or decode the srcs of the validation_set and test_set.\n",
    "            Defaults to None.\n",
    "        tgt_vocab (Vocabulary): Vocabulary to encode or decode the tgts of the validation_set and test_set.\n",
    "            Defaults to None.\n",
    "        with_text (bool): Whether to include raw text in the output when calling __getitem__ method.\n",
    "            It is used in evaluation and reinforcement learning. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        srcs: List[str],\n",
    "        tgts: List[str],\n",
    "        min_freq: int = 1,\n",
    "        max_vocab: int = 99999,\n",
    "        src_delimiter: str = \" \",\n",
    "        tgt_delimiter: str = \" \",\n",
    "        src_vocab: Union[Type[Vocabulary], NoneType] = None,\n",
    "        tgt_vocab: Union[Type[Vocabulary], NoneType] = None,\n",
    "        with_text: bool = False,\n",
    "    ):\n",
    "        # Originally, srcs and tgts both must have been tokenized using BPE before.\n",
    "        # But in agri translation model, tgts were tokenized with custom tokenization.\n",
    "        # Instead, tgts have to be delimited by tgt_delimiter before.\n",
    "        self.srcs, self.tgts = srcs, tgts\n",
    "        self.src_delimiter, self.tgt_delimiter = src_delimiter, tgt_delimiter\n",
    "\n",
    "        # If with_text is True, not only the encoded_src and encoded_tgt,\n",
    "        # the raw src and tgt text would be returned together when __getitem__ is called.\n",
    "        self.with_text = with_text\n",
    "\n",
    "        # If the Dataset is train_dataset, it has to build its vocabulary.\n",
    "        if src_vocab is None or tgt_vocab is None:\n",
    "            # Initialize vocabulary of sources and targets\n",
    "            self.src_vocab = Vocabulary(min_freq=min_freq, max_vocab=max_vocab)\n",
    "            self.tgt_vocab = Vocabulary(min_freq=min_freq, max_vocab=max_vocab)\n",
    "            # Build vocabulary of sources and targets\n",
    "            self.src_vocab.build_vocab(self.srcs, delimiter=src_delimiter)\n",
    "            self.tgt_vocab.build_vocab(self.tgts, delimiter=tgt_delimiter)\n",
    "        else:\n",
    "            # If the Dataset is validation or test_dateset, it has to use the vocabulary originated from train_dataset.\n",
    "            self.src_vocab = src_vocab\n",
    "            self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.srcs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.srcs[idx], self.tgts[idx]\n",
    "\n",
    "        # encode src\n",
    "        encoded_src = self.src_vocab.encode(src, delimiter=self.src_delimiter)\n",
    "\n",
    "        # In seq2seq structure, tgt must have BOS and EOS token at the beginning and the end.\n",
    "        encoded_tgt = self.tgt_vocab.encode(tgt, delimiter=self.tgt_delimiter)\n",
    "        encoded_tgt.insert(0, Vocabulary.BOS)\n",
    "        encoded_tgt.append(Vocabulary.EOS)\n",
    "\n",
    "        return_value = {\n",
    "            \"src\": torch.tensor(encoded_src),\n",
    "            \"tgt\": torch.tensor(encoded_tgt),\n",
    "        }\n",
    "\n",
    "        # src_txt and tgt_txt would be used in inference and evaluation\n",
    "        if self.with_text:\n",
    "            return_value[\"src_text\"] = src\n",
    "            return_value[\"tgt_text\"] = tgt\n",
    "\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c40518bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies:  53430\n",
      "Number of vocabularies:  30488\n"
     ]
    }
   ],
   "source": [
    "Dataset = TranslationDataset(\n",
    "    srcs=train_data[\"tok_ko\"].tolist(), tgts=train_data[\"tok_en\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7b61025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281934"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2881f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁잠 잘 ▁때 ▁모습이 ▁어떻게 ▁저렇게 ▁웃 길 ▁수 ▁있죠 ?']\n",
      "['▁How ▁can ▁one ▁look ▁so ▁funny ▁while ▁sleeping ?']\n"
     ]
    }
   ],
   "source": [
    "print(Dataset.src_vocab.decode(Dataset[170000][\"src\"], delimiter=\" \"))\n",
    "print(Dataset.tgt_vocab.decode(Dataset[170000][\"tgt\"], delimiter=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b81f6bb-e8d4-4cbe-bc57-e8434392626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TranslationCollator:\n",
    "    def __init__(self, pad_idx: int, max_length: int, with_text: bool = False):\n",
    "        \"\"\"\n",
    "        Usages:\n",
    "            It is used as a parameter in DataLoader.\n",
    "            Collate batch srcs or tgts and process it to make batch loader.\n",
    "            Add length of each src and tgt, and add pad token according to the length of batch.\n",
    "        Args:\n",
    "            pad_idx (int): Index of pad_token.\n",
    "            max_length (list): Max length of the encoded_srcs or encoded_tgts .\n",
    "            with_text (bool): Whether to include raw text in the output.\n",
    "        \"\"\"\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_length = max_length\n",
    "        self.with_text = with_text\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # |batch| = [{\"src\": tensor[], \"tgt\": tensor[]}, {\"src\": tensor[], \"tgt\": tensor[]}...]\n",
    "\n",
    "        srcs, tgts = [], []\n",
    "\n",
    "        # If there are raw text passed from batch, include them in the returned value\n",
    "        # If length of src or target is larger than max_length, truncate it.\n",
    "        if self.with_text:\n",
    "            srcs_texts, tgts_texts = [], []\n",
    "\n",
    "            for sample in batch:\n",
    "                srcs.append(\n",
    "                    (\n",
    "                        sample[\"src\"][: self.max_length],\n",
    "                        len(sample[\"src\"][: self.max_length]),\n",
    "                    )\n",
    "                )\n",
    "                tgts.append(\n",
    "                    (\n",
    "                        sample[\"tgt\"][: self.max_length],\n",
    "                        len(sample[\"tgt\"][: self.max_length]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                srcs_texts.append(\n",
    "                    \" \".join(sample[\"src_text\"].split(\" \")[: self.max_length])\n",
    "                )\n",
    "                tgts_texts.append(\n",
    "                    \" \".join(sample[\"tgt_text\"].split(\" \")[: self.max_length])\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            for sample in batch:\n",
    "                srcs.append(\n",
    "                    (\n",
    "                        sample[\"src\"][: self.max_length],\n",
    "                        len(sample[\"src\"][: self.max_length]),\n",
    "                    )\n",
    "                )\n",
    "                tgts.append(\n",
    "                    (\n",
    "                        sample[\"tgt\"][: self.max_length],\n",
    "                        len(sample[\"tgt\"][: self.max_length]),\n",
    "                    )\n",
    "                )\n",
    "        # |srcs| = [(src_ids, src_length), (src_ids, src_length) ...]\n",
    "        # |srcs_texts| = [src_text, src_text, ...]\n",
    "\n",
    "        # Pad Sequence with pad token according to the length\n",
    "        srcs, srcs_lengths = zip(*srcs)\n",
    "        tgts, tgts_lengths = zip(*tgts)\n",
    "        # |srcs| = [[src_ids], [src_ids] ...]\n",
    "        # |srcs_lenghts| = [src_length, src_length]\n",
    "\n",
    "        srcs = pad_sequence(srcs, batch_first=True, padding_value=self.pad_idx)\n",
    "        tgts = pad_sequence(tgts, batch_first=True, padding_value=self.pad_idx)\n",
    "        # |srcs| = (batch_size, batch_max_length)\n",
    "\n",
    "        srcs = (srcs, torch.LongTensor(srcs_lengths))\n",
    "        tgts = (tgts, torch.LongTensor(tgts_lengths))\n",
    "\n",
    "        return_value = {\n",
    "            \"input_ids\": srcs,\n",
    "            \"output_ids\": tgts,\n",
    "        }\n",
    "\n",
    "        if self.with_text:\n",
    "            return_value[\"input_texts\"] = srcs_texts\n",
    "            return_value[\"output_texts\"] = tgts_texts\n",
    "\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1893d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_srcs = train_data[\"tok_ko\"].tolist()\n",
    "train_tgts = train_data[\"tok_en\"].tolist()\n",
    "\n",
    "test_srcs = []\n",
    "test_tgts = []\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "for src, tgt in zip(train_srcs, train_tgts):\n",
    "    if len(src.split(\" \")) > MAX_LENGTH or len(tgt.split(\" \")) > MAX_LENGTH:\n",
    "        test_srcs += [src]\n",
    "        test_tgts += [tgt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e04fdcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies:  1314\n",
      "Number of vocabularies:  1421\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    TranslationDataset(test_srcs, test_tgts, with_text=True),\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=TranslationCollator(\n",
    "        pad_idx=Vocabulary.PAD, max_length=MAX_LENGTH, with_text=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79400b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9b83359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': (tensor([[599, 600, 207, 214,   5, 293, 601, 602,  14, 603, 604, 605,  99, 606,\n",
       "            21, 607,   4, 125, 608,  95, 609, 125, 610,  22, 611, 215, 612, 216,\n",
       "           613,  27,   4, 163, 214, 614, 615,   8, 616, 217, 218, 617, 618, 619,\n",
       "           620,   5, 621,   5, 622, 129, 623,   5, 624, 121, 130, 625, 294,   4,\n",
       "           125, 214,   5, 293, 626, 627,  54, 628, 162, 295, 103,  11]]),\n",
       "  tensor([68])),\n",
       " 'output_ids': (tensor([[  1, 695,   9, 696, 697, 271, 698,   4, 699, 418, 369,  52,   4, 419,\n",
       "           700, 701,   7, 702, 276,  92,   7,   4, 132,   5,  97,   7, 134,   6,\n",
       "           109,  27,   4,  66,  40,   4, 136,  16, 703,  90,   9,  67, 131,   6,\n",
       "           108, 420,   9, 704,   6, 277, 705,   5,  90,   9,  67, 131, 156,  41,\n",
       "           162, 706, 421,   9, 707,   6, 112,   5, 422,   7, 708,  41,   6, 278,\n",
       "            35, 163, 709,  14, 423,   7, 710, 711, 106, 712, 418,   6,  55,  21,\n",
       "           713,   6, 714,   6,   7, 715, 716,   8, 717, 718, 719,  14, 720, 721,\n",
       "           279,   8, 722, 423,  14, 723,   6, 278,  35, 424, 724,   9, 370,   6,\n",
       "           277, 725,   5,  90,   9,  67, 131, 419, 726,   7, 727, 276, 728,  92,\n",
       "             6, 271]]),\n",
       "  tensor([128])),\n",
       " 'input_texts': ['▁현장 소통 에는 ▁저출산 · 고령사회 위원회의 ▁민간위원 과 ▁정부 ▁간사 부처 인 ▁보건복지부 가 ▁참여하며 , ▁경기도 에서는 ▁이 재율 ▁경기도 ▁행정 1 부지사 와 ▁조정 아 ▁여성가족 과장 , ▁지역 ▁저출산 ▁인식개선 활동 의 ▁구 심 적 ▁역할을 ▁위해 ▁구성된 ▁종교 · 언론 · 시민단체 ▁등의 ▁출산 · 육아 지원 협의회 ▁참여 단체 , ▁경기도 ▁저출산 · 고령사회 대책위원회 ▁최진 호 ▁부위원장 ▁등이 ▁함께 한다 .'],\n",
       " 'output_texts': ['▁On - site ▁communication ▁will ▁involve ▁the ▁private ▁committee ▁members ▁from ▁the ▁Low ▁Fer tility ▁and ▁Aging ▁Society ▁Committee ▁and ▁the ▁Ministry ▁of ▁Health ▁and ▁Welfare , ▁which ▁is ▁the ▁department ▁under ▁the ▁government . ▁From ▁Gyeonggi - do ▁Province , ▁Lee ▁Jae - yul , ▁Vice ▁Governor ▁of ▁Gyeonggi - do ▁Province ▁Administration ▁Division ▁1, ▁Jo ▁Jeong - ah , ▁Director ▁of ▁Women ▁and ▁Family ▁Division , ▁along ▁with ▁organizations ▁participating ▁in ▁birth ▁and ▁child ▁raising ▁support ▁negotiation ▁committee , ▁such ▁as ▁religious , ▁press , ▁and ▁civic ▁organization ▁to ▁play ▁main ▁role ▁in ▁changing ▁thoughts ▁due ▁to ▁low ▁birth ▁in ▁region , ▁along ▁with ▁Choi ▁Jin - ho , ▁Vice ▁chairman ▁of ▁Gyeonggi - do ▁Province ▁Low ▁Birth ▁and ▁Elderly ▁Society ▁Measurement ▁Committee , ▁will ▁participate']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c87a711c8c6bd3ac185ec2dd9171ed185e279191e75d73d0e6b65c559dd8b5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
