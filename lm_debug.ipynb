{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from chameleon.dl_dataset import Vocabulary, TranslationDataset, TranslationCollator\n",
    "from chameleon.models.lstm_lm import LanguageModel\n",
    "from argparse import Namespace\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # load data\n",
    "\n",
    "    train_data = pd.read_pickle(DATA_ROOT + \"chameleon.train.tok.pickle\")\n",
    "    valid_data = pd.read_pickle(DATA_ROOT + \"chameleon.valid.tok.pickle\")\n",
    "    test_data = pd.read_pickle(DATA_ROOT + \"chameleon.test.tok.pickle\")\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data/\"\n",
    "train_data, valid_data, test_data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"lang\": \"enko\",\n",
    "        \"gpu_id\": 0,\n",
    "        \"use_mps\": True,\n",
    "        \"batch_size\": 16,\n",
    "        \"n_epochs\": 2,\n",
    "        \"max_length\": 512,\n",
    "        \"dropout\": 0.2,\n",
    "        \"word_vec_size\": 512,\n",
    "        \"hidden_size\": 768,\n",
    "        \"n_layers\": 4,\n",
    "        \"max_grad_norm\": 1e8,\n",
    "    }\n",
    "    config = Namespace(**config)\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source language: en, target language: ko\n",
      "Number of vocabularies:  30488\n",
      "Number of vocabularies:  53430\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_loaders(config):\n",
    "    # specify source and target language\n",
    "    src_lang = config.lang[:2]\n",
    "    tgt_lang = config.lang[-2:]\n",
    "    print(f\"source language: {src_lang}, target language: {tgt_lang}\")\n",
    "\n",
    "    # DataLoader for source and target language\n",
    "    train_loader = DataLoader(\n",
    "        TranslationDataset(\n",
    "            srcs=train_data[f\"tok_{src_lang}\"].tolist(),\n",
    "            tgts=train_data[f\"tok_{tgt_lang}\"].tolist(),\n",
    "            with_text=False,\n",
    "            is_dual=True,  # tgt dataset also needs BOS and EOS token at the begging and the end.\n",
    "        ),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=TranslationCollator(\n",
    "            pad_idx=Vocabulary.PAD,\n",
    "            eos_idx=Vocabulary.EOS,\n",
    "            max_length=config.max_length,\n",
    "            with_text=False,\n",
    "            is_dual=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    train_src_vocab = train_loader.dataset.src_vocab\n",
    "    train_tgt_vocab = train_loader.dataset.tgt_vocab\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        TranslationDataset(\n",
    "            srcs=valid_data[f\"tok_{src_lang}\"].tolist(),\n",
    "            tgts=valid_data[f\"tok_{tgt_lang}\"].tolist(),\n",
    "            src_vocab=train_src_vocab,\n",
    "            tgt_vocab=train_tgt_vocab,\n",
    "            with_text=False,\n",
    "            is_dual=True,\n",
    "        ),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=TranslationCollator(\n",
    "            pad_idx=Vocabulary.PAD,\n",
    "            eos_idx=Vocabulary.EOS,\n",
    "            max_length=config.max_length,\n",
    "            with_text=False,\n",
    "            is_dual=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "train_loader, valid_loader = get_loaders(config)\n",
    "\n",
    "## ----- Test data_loader ----- #\n",
    "# # input_ids would be dataset for source LM\n",
    "# # output_ids would be dataset for target LM\n",
    "# test_batch = next(iter(train_loader))\n",
    "# print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary size for source and target language\n",
    "src_vocab_size, tgt_vocab_size = (\n",
    "    len(train_loader.dataset.src_vocab),\n",
    "    len(train_loader.dataset.tgt_vocab),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get LanguageModel for source language only here\n",
    "# later in actual implementation,\n",
    "# we are going to load two LMs, each for source and target\n",
    "def get_model(config):\n",
    "    src_model = LanguageModel(\n",
    "        vocab_size=src_vocab_size,\n",
    "        word_vec_size=config.word_vec_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        n_layers=config.n_layers,\n",
    "        dropout=config.dropout,\n",
    "        max_length=config.max_length,\n",
    "    )\n",
    "    return src_model\n",
    "\n",
    "\n",
    "src_model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (emb): Embedding(30488, 512, padding_idx=0)\n",
      "  (rnn): LSTM(512, 768, num_layers=4, batch_first=True, dropout=0.2)\n",
      "  (out): Linear(in_features=768, out_features=30488, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(src_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crit(src_vocab_size, pad_idx):\n",
    "    \"\"\"[TODO]\n",
    "    If training the LMs at once,\n",
    "    we need to return two criterions.\n",
    "    \"\"\"\n",
    "    loss_weight = torch.ones(src_vocab_size)\n",
    "    loss_weight[pad_idx] = 0.0\n",
    "\n",
    "    crit = nn.NLLLoss(weight=loss_weight, reduction=\"none\")\n",
    "\n",
    "    return crit\n",
    "\n",
    "\n",
    "crit = get_crit(src_vocab_size, Vocabulary.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model to gpu\n",
    "if config.gpu_id >= 0 and not config.use_mps:\n",
    "    src_model.cuda(config.gpu_id)\n",
    "    # Reason we need to load the criterion to GPU\n",
    "    # https://discuss.pytorch.org/t/move-the-loss-function-to-gpu/20060/5\n",
    "    # A weight parameter could be seen as an internal state and would yield a device mismatch error.\n",
    "    # Of course you might define the weight parameter as a CUDATensor, but you could also move the criterion to the device\n",
    "    crit.cuda(config.gpu_id)\n",
    "elif config.use_mps:\n",
    "    src_model.to(\"mps:{}\".format(config.gpu_id))\n",
    "    crit.to(\"mps:{}\".format(config.gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(src_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimizer\n",
    "optimizer = optim.Adam(src_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tqdm ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# n = 1000\n",
    "# i = 0\n",
    "# while i<n:\n",
    "#     time.sleep(0.1)\n",
    "#     value = np.random.randint(1, 100)\n",
    "#     i += value\n",
    "#     print(i, end = \"\\r\")\n",
    "\n",
    "# print(\"Loop Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# n = 1000\n",
    "# i = 0\n",
    "# pbar = tqdm(desc=\"while loop\", total=n)\n",
    "# while i<n:\n",
    "#     time.sleep(0.1)\n",
    "#     value = np.random.randint(1, 100)\n",
    "#     i += value\n",
    "#     pbar.update(value)\n",
    "#     pbar.set_postfix(loss = i)\n",
    "#     # print(i, end = \"\\r\")\n",
    "\n",
    "# print(\"Loop Completed.\")\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration = 1000\n",
    "# pbar = tqdm(range(iteration), desc=\"Epoch - 1\", total = iteration)\n",
    "# loss = 0\n",
    "# for i in pbar:\n",
    "#     loss += 2\n",
    "#     pbar.set_postfix(loss = loss)\n",
    "# pbar.close()\n",
    "# print(f\"Epoch - {loss / 30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_batch = next(iter(train_loader))\n",
    "# src_model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     device = next(src_model.parameters()).device\n",
    "#     test_batch[\"input_ids\"] = (\n",
    "#         test_batch[\"input_ids\"][0].to(device),\n",
    "#         test_batch[\"input_ids\"][1]\n",
    "#     )\n",
    "\n",
    "#     x = test_batch[\"input_ids\"][0][:, :-1]\n",
    "#     y = test_batch[\"input_ids\"][0][:, 1:]\n",
    "#     print(x.size(), y.size())\n",
    "\n",
    "#     # forward\n",
    "#     y_hat = src_model(x)\n",
    "#     # |y_hat| = (batch_size, length, output_size)\n",
    "#     print(y_hat)\n",
    "#     print(y_hat.size())\n",
    "\n",
    "#     # calculate loss\n",
    "#     loss = crit(\n",
    "#         y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "#         y.contiguous().view(-1),\n",
    "#     ).sum() # criterion - reduction = None\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Trainer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        crit,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        src_vocab,\n",
    "        tgt_vocab,\n",
    "        config,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.config = config\n",
    "\n",
    "        self.latest_loss = None\n",
    "        self.best_loss = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        # # initialize pbar\n",
    "        # pbar = tqdm(\n",
    "        #     self.train_loader,\n",
    "        #     desc=f\"Training Epoch - {epoch}\",\n",
    "        #     total = len(self.train_loader)\n",
    "        # )\n",
    "\n",
    "        total_loss = 0\n",
    "        for idx, mini_batch in enumerate(self.train_loader):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            mini_batch[\"input_ids\"] = (\n",
    "                mini_batch[\"input_ids\"][0].to(device),\n",
    "                mini_batch[\"input_ids\"][1],\n",
    "            )\n",
    "\n",
    "            x = mini_batch[\"input_ids\"][0][:, :-1]\n",
    "            y = mini_batch[\"input_ids\"][0][:, 1:]\n",
    "\n",
    "            # forward\n",
    "            y_hat = self.model(x)\n",
    "            # |y_hat| = (batch_size, length, output_size)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = self.crit(\n",
    "                y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                y.contiguous().view(-1),\n",
    "            ).sum()  # criterion - reduction = None\n",
    "            backward_target = loss.div(y.size(0))\n",
    "\n",
    "            # backward loss\n",
    "            backward_target.backward()\n",
    "\n",
    "            word_count = int(mini_batch[\"input_ids\"][1].sum())\n",
    "\n",
    "            # graient clipping\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "\n",
    "            # update model parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss = float(loss / word_count)\n",
    "            ppl = np.exp(loss)\n",
    "\n",
    "            total_loss += float(loss)\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch - {epoch} - {idx+1}/{len(self.train_loader)} - loss: {loss}, ppl: {ppl}\"\n",
    "                )\n",
    "\n",
    "            # # update pbar\n",
    "            # pbar.set_postfix(loss = loss, ppl = ppl)\n",
    "\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def _validate(self, epoch):\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        # # initialize pbar\n",
    "        # pbar = tqdm(\n",
    "        #     self.train_loader,\n",
    "        #     desc=f\"Validation Epoch - {epoch}\",\n",
    "        #     total = len(self.valid_loader)\n",
    "        # )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for idx, mini_batch in enumerate(self.valid_loader):\n",
    "                self.model.eval()\n",
    "                mini_batch[\"input_ids\"] = (\n",
    "                    mini_batch[\"input_ids\"][0].to(device),\n",
    "                    mini_batch[\"input_ids\"][1],\n",
    "                )\n",
    "\n",
    "                x = mini_batch[\"input_ids\"][0][:, :-1]\n",
    "                y = mini_batch[\"input_ids\"][0][:, 1:]\n",
    "\n",
    "                # forward\n",
    "                y_hat = self.model(x)\n",
    "                # |y_hat| = (batch_size, length, output_size)\n",
    "\n",
    "                # calculate loss\n",
    "                loss = self.crit(\n",
    "                    y_hat.contiguous().view(-1, y_hat.size(-1)), y.contiguous().view(-1)\n",
    "                ).sum()  # criterion - reduction = 'none'\n",
    "\n",
    "                word_count = int(mini_batch[\"input_ids\"][1].sum())\n",
    "                loss = float(loss / word_count)\n",
    "                ppl = np.exp(loss)\n",
    "\n",
    "                total_loss += float(loss)\n",
    "\n",
    "                # pbar.set_postfix(loss = loss, ppl = ppl)\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch - {epoch} - {idx+1}/{len(self.valid_loader)} - loss: {loss}, ppl: {ppl}\"\n",
    "                    )\n",
    "\n",
    "        return total_loss / len(self.valid_loader)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(config.n_epochs):\n",
    "            train_loss = self._train(epoch)\n",
    "            valid_loss = self._validate(epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch - {epoch+1}: train_loss: {train_loss} ; valid_loss: {valid_loss}\"\n",
    "            )\n",
    "\n",
    "            # update latest_loss\n",
    "            self.latest_loss = valid_loss\n",
    "\n",
    "            # check best model and copy it to self.model\n",
    "            self.check_best()\n",
    "\n",
    "        # Restore to best model.\n",
    "        self.model.load_state_dict(self.best_model)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def check_best(self):\n",
    "        loss = self.latest_loss\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        if loss <= self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "\n",
    "\n",
    "LMTrainer = Trainer(\n",
    "    model=src_model,\n",
    "    crit=crit,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    src_vocab=None,\n",
    "    tgt_vocab=None,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "model = LMTrainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
