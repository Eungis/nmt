{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chameleon.base_dataset import Vocabulary, TranslationDataset, TranslationCollator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def read_data(file_path, src_tgt):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    src_lang, tgt_lang = src_tgt[:2], src_tgt[2:]\n",
    "\n",
    "    # parse source column and target column\n",
    "    src_col, tgt_col = (\"tok\" + \"_\" + src_lang, \"tok\" + \"_\" + tgt_lang)\n",
    "    srcs = data[src_col].tolist()\n",
    "    tgts = data[tgt_col].tolist()\n",
    "    return srcs, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-14 11:53:33.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[35mNone\u001b[0m | \u001b[36mchameleon.base_dataset\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mNumber of vocabularies: 30488\u001b[0m\n",
      "\u001b[32m2023-08-14 11:53:38.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[35mNone\u001b[0m | \u001b[36mchameleon.base_dataset\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mNumber of vocabularies: 53430\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_srcs, train_tgts = read_data(\"./data/chameleon.train.tok.pickle\", \"enko\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TranslationDataset(train_srcs, train_tgts, with_text=True),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    collate_fn=TranslationCollator(\n",
    "        pad_idx=Vocabulary.PAD, max_length=256, with_text=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model forward functionality\n",
    "- Data: Using first batch of the input, output data\n",
    "- `Encoder` Test\n",
    "- `Decoder` Test\n",
    "- `Attention` Test\n",
    "- `Generator` Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 42])\n",
      "torch.Size([5, 48])\n",
      "{'input_ids': (tensor([[   19,   298,    12, 13774,   961,    28, 15242,    11,  5100,   134,\n",
      "            24,  7420,  5045,     7,   122,    12,  3568,   829,     5,   433,\n",
      "           124,   167,    17,   316,     8,   914,     6,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [   20,    15,   276,   895,   217,    20,  2411,    31,  4093,     8,\n",
      "           791,    22,   333,    30,     4, 11711,   791,   286,     6,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [   20,   515,  2944,   396,    21,    32,   242,    15,    57,   515,\n",
      "          4580,  2488,     9,   106,     6,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 4968,    12,   114,    48,    16,   181,  3744,    11,     4,   440,\n",
      "            56,   571,    17,    39,    54,    15,    14,  2781,   906,   138,\n",
      "           195,  1302,    26,     4,   188,     7,  3192,     5,  1652,     8,\n",
      "          3738,   861,     8,  2513,    10,   195,   640,     7,  8064,   201,\n",
      "            99,     6],\n",
      "        [ 6915,    15,    14,   235,    12, 16623,   112,  1120,     5, 12884,\n",
      "          4624,     5,    13,  1263,   457,    85, 18613,  1574,     5,   629,\n",
      "             4,  1200,  3558,    47,  9399,    45,    78,     7,   414,    12,\n",
      "          2611,  3336,    18,  2626, 15047,     6,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([27, 19, 15, 42, 36])), 'output_ids': (tensor([[    1,    92,   955, 16963,    31,  9478,  1030,   353,  3866,  3093,\n",
      "         15181,     6, 18579, 18545, 11491, 30851,  2518,   443,   688,  4891,\n",
      "         17443,  2440,   170,  2082,     4,     2,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   396,    30, 22916,   157,    63, 33288,    56, 19972, 25776,\n",
      "          2034,    64,  3679,     4,     2,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   681,  9057, 25796,   470, 38709,   208, 12428,  1456,  3501,\n",
      "          3892,   169,   778,     4,     2,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1, 20118,    10, 11280,     9, 10499,   180,  4030,  3457,  4140,\n",
      "            12,   639,    70,    81,  7238, 11098,    41,   256,    62,  7768,\n",
      "            49,   318,     6,  3634,     8,  8990,    19,    71,     4,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  6327,  7244,     5, 10398,  7842,    29,  1613,  1718,  3239,\n",
      "            24,     6,  2347, 16633, 43237,     9,  1931, 19578,  2741, 11612,\n",
      "          9848,    30,   512,   152,  2259, 11116,    66,    95,   246,  3563,\n",
      "            29, 10290,   368,    24,     8,  6297,   231,  6587,  3832,    53,\n",
      "           258,   562,     8, 12634,    56,    15,     4,     2]]), tensor([26, 15, 15, 30, 48])), 'input_texts': ['▁The ▁6 - pack ▁items ▁are ▁packaged ▁in ▁boxes ▁made ▁with ▁lovely ▁designs ▁of ▁high - quality ▁materials , ▁making ▁them ▁good ▁for ▁family ▁and ▁friends .', \"▁I ' ll ▁review ▁what ▁I ▁learned ▁from ▁Hana ▁and ▁study ▁it ▁again ▁at ▁the ▁Itaewon ▁study ▁group .\", \"▁I ▁feel ▁blue ▁too ▁as ▁you ▁don ' t ▁feel ▁excited ▁talking ▁to ▁me .\", \"▁Chungbuk - do ▁said ▁that ▁10 ▁festivals ▁in ▁the ▁province ▁were ▁selected ▁for ▁this ▁year ' s ▁rural ▁festival ▁support ▁project ▁organized ▁by ▁the ▁Ministry ▁of ▁Agriculture , ▁Food ▁and ▁Rural ▁Affairs ▁and ▁secured ▁a ▁project ▁cost ▁of ▁140 ▁million ▁won .\", \"▁Psy ' s ▁long - awaited ▁new ▁song , ▁Dad dy , ▁is ▁showing ▁off ▁its ▁enduring ▁popularity , ▁taking ▁the ▁highest ▁ranking ▁( 54 th ) ▁of ▁K - Pop ▁singers ▁on ▁Indonesia ▁iTunes .\"], 'output_texts': ['▁6 개의 ▁팩 으로 ▁이루어진 ▁상품 들은 ▁고급 스러운 ▁재질 의 ▁사랑스러운 ▁디자인으로 ▁제작된 ▁박스에 ▁포장 되어 ▁있어서 ▁가족과 ▁친구들에게 ▁선물 하기 ▁좋다 .', '▁하나 에서 ▁배웠 던 ▁것 ▁복습 하고 ▁이태원 ▁스터디 ▁공부 ▁할 ▁거에요 .', '▁네가 ▁나랑 ▁얘기하는 ▁거 ▁재미없 어 하니까 ▁나도 ▁기분이 ▁울 적 하다 .', '▁충북도 는 ▁농림축산식품부 가 ▁주관한 ▁올해 ▁농촌 축제 ▁지원사업 에 ▁도내 ▁10 개 ▁축제가 ▁선정돼 ▁1 억 ▁4 400 만 ▁원 의 ▁사업비 를 ▁확보했다 고 ▁밝혔다 .', '▁오랜만에 ▁신곡 을 ▁출시한 ▁싸이 ( P s y ) 의 ▁D ad dy 가 ▁인도네시아 ▁i T un es 에서 ▁K - Pop ▁가수들 ▁중 ▁가장 ▁높은 ▁순위 ( 54 위 ) 를 ▁차지 하면서 ▁녹 슬 지 ▁않은 ▁인기 를 ▁과시 하고 ▁있다 .']}\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"input_ids\"][0].size())\n",
    "print(batch[\"output_ids\"][0].size())\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"input_ids\"]\n",
    "y = batch[\"output_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if isinstance(x, tuple):\n",
    "    x, x_length = x\n",
    "\n",
    "if isinstance(y, tuple):\n",
    "    y = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(train_loader.dataset.src_vocab)\n",
    "output_size = len(train_loader.dataset.tgt_vocab)\n",
    "hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 40, 768])\n",
      "torch.Size([5, 40, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embedding x\n",
    "\n",
    "\n",
    "def generate_pos_enc(hidden_size, max_length):\n",
    "    enc = torch.FloatTensor(max_length, hidden_size).zero_()\n",
    "    # |enc| = (max_length, hidden_size)\n",
    "\n",
    "    pos = torch.arange(0, max_length).unsqueeze(-1).float()\n",
    "    dim = torch.arange(0, hidden_size // 2).unsqueeze(0).float()\n",
    "    # |pos| = (max_length, 1)\n",
    "    # |dim| = (1, hidden_size // 2)\n",
    "    # |pos / dim| = (max_length, hidden_size // 2)\n",
    "    enc[:, 0::2] = torch.sin(pos / 1e4 ** dim.div(float(hidden_size)))\n",
    "    enc[:, 1::2] = torch.cos(pos / 1e4 ** dim.div(float(hidden_size)))\n",
    "\n",
    "    return enc\n",
    "\n",
    "\n",
    "def encode_position(x, pos_enc, max_length, init_pos=0):\n",
    "    # |x| = (batch_size, n, hidden_size)\n",
    "    # |pos_enc| = (max_length, hidden_size)\n",
    "    assert x.size(-1) == pos_enc.size(-1)\n",
    "    assert x.size(1) + init_pos <= max_length\n",
    "\n",
    "    pos_enc = pos_enc[init_pos : init_pos + x.size(1)].unsqueeze(0)\n",
    "    # |pos_enc| = (1, n, hidden_size)\n",
    "\n",
    "    x = x + pos_enc.to(x.device)\n",
    "    # broadcasting\n",
    "    # |x| = (batch_size, n, hidden_size)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Embedding Layer\n",
    "emb_src = nn.Embedding(input_size, hidden_size)\n",
    "emb_tgt = nn.Embedding(output_size, hidden_size)\n",
    "emb_x = emb_src(x)\n",
    "print(emb_x.size())\n",
    "\n",
    "max_length = 512\n",
    "pos_enc = generate_pos_enc(hidden_size, max_length)\n",
    "emb_x = encode_position(emb_x, pos_enc, max_length)\n",
    "print(emb_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(x, length):\n",
    "    mask = []\n",
    "\n",
    "    max_length = max(length)\n",
    "    for l in length:\n",
    "        if max_length - l > 0:\n",
    "            # If the length is shorter than maximum length among samples,\n",
    "            # set last few values to be 1s to remove attention weight.\n",
    "            mask += [\n",
    "                torch.cat(\n",
    "                    [x.new_ones(1, l).zero_(), x.new_ones(1, (max_length - l))], dim=-1\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            # If the length of the sample equals to maximum length among samples,\n",
    "            # set every value in mask to be 0.\n",
    "            mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "    mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "mask = generate_mask(x, x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 40])\n",
      "torch.Size([5, 40])\n"
     ]
    }
   ],
   "source": [
    "print(mask.size())\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mask_enc = mask.unsqueeze(1).expand(x.size(0), x.size(1), mask.size(-1))\n",
    "print(mask_enc.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None, dk=64):\n",
    "        # |Q| = (batch_size, m, hidden_size) # |Q| = (n_splits * batch_size, m, hidden_size / n_splits)\n",
    "        # |K|, |V| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        w = torch.bmm(Q, K.transpose(1, 2))\n",
    "        # |w| = (batch_size, m, n)\n",
    "\n",
    "        if mask is not None:\n",
    "            assert w.size() == mask.size()\n",
    "            w.masked_fill_(mask, -float(\"inf\"))\n",
    "        w = self.softmax(w / (dk**0.5))\n",
    "        c = torch.bmm(w, V)\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return c\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, hidden_size, n_splits):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "        self.Q_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.attn = Attention()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # |Q| = (batch_size, m, hidden_size)\n",
    "        # |K| = (batch_size, n, hidden_size)\n",
    "        # |V| = |K|\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        QWs = self.Q_linear(Q).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        KWs = self.K_linear(Q).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        VWs = self.V_linear(Q).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        # |QW_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        # |KW_i| = (batch_size, n, hidden_size / n_splits)\n",
    "\n",
    "        QWs = torch.cat(QWs, dim=0)\n",
    "        KWs = torch.cat(KWs, dim=0)\n",
    "        VWs = torch.cat(VWs, dim=0)\n",
    "        # |QWs| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "        # |KWs| = (batch_size * n_splits, n, hidden_size / n_splits)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = torch.cat([mask for _ in range(self.n_splits)], dim=0)\n",
    "        # |mask| = (batch_size * n_splits, m, n)\n",
    "\n",
    "        c = self.attn(QWs, KWs, VWs, mask=mask, dk=self.hidden_size // self.n_splits)\n",
    "        # |c| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "        c = c.split(Q.size(0), dim=0)\n",
    "        # |c_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        c = self.linear(torch.cat(c, dim=-1))\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention\n",
    "- before making `EncoderBlock`\n",
    "- after making `EncoderBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead attention\n",
    "n_splits = 8\n",
    "attn = MultiHead(hidden_size, n_splits)\n",
    "attn_norm = nn.LayerNorm(hidden_size)\n",
    "attn_dropout = nn.Dropout(0.2)\n",
    "\n",
    "fc_norm = nn.LayerNorm(hidden_size)\n",
    "fc_dropout = nn.Dropout(0.2)\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(hidden_size, hidden_size * 4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(hidden_size * 4, hidden_size),\n",
    ")\n",
    "\n",
    "# pre Linear Normalization\n",
    "\n",
    "# linear normalization\n",
    "z = attn_norm(emb_x)\n",
    "# multihead attention\n",
    "z = attn(Q=z, K=z, V=z, mask=mask_enc)\n",
    "## |z| = (batch_size, n, hidden_size)\n",
    "z = attn_dropout(z)\n",
    "# residual connection\n",
    "z = z + emb_x\n",
    "\n",
    "# fc normalization\n",
    "z = fc_norm(z)\n",
    "# fully connected layer with LeakyReLU\n",
    "z = fc(z)\n",
    "# dropout\n",
    "z = fc_dropout(z)\n",
    "# |z| = (batch_size, n, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, n_splits, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHead(hidden_size, n_splits)\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        z = self.attn_norm(x)\n",
    "        z = x + self.attn_dropout(self.attn(Q=z, K=z, V=z, mask=mask))\n",
    "        z = z + self.fc_dropout(self.fc(self.fc_norm(z)))\n",
    "        return z, mask\n",
    "\n",
    "\n",
    "class CustomSequential(nn.Sequential):\n",
    "    def forward(self, *x):\n",
    "        for module in self._modules.values():\n",
    "            x = module(*x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_block = CustomSequential(\n",
    "    *[EncoderBlock(hidden_size, n_splits) for _ in range(2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _ = encoder_block(emb_x, mask_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 40, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention\n",
    "- making `DecoderBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding y\n",
    "emb_dropout = nn.Dropout(0.2)\n",
    "emb_y = emb_tgt(y)\n",
    "emb_y = encode_position(emb_y, pos_enc, max_length)\n",
    "emb_y = emb_dropout(emb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate future mask\n",
    "with torch.no_grad():\n",
    "    future_mask = torch.triu(\n",
    "        emb_x.new_ones(emb_y.size(1), emb_y.size(1)), diagonal=1\n",
    "    ).bool()\n",
    "    # |future_mask| = (m, m)\n",
    "    future_mask = future_mask.unsqueeze(0).expand(emb_y.size(0), *future_mask.size())\n",
    "    # |future_mask| = (batch_size, m, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51, 51])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, n_splits, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Two types of attention\n",
    "        self.masked_attn = MultiHead(hidden_size, n_splits)\n",
    "        self.masked_attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.masked_attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn = MultiHead(hidden_size, n_splits)\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_and_value, mask, prev, future_mask):\n",
    "        # |key_and_value| = (batch_size, m, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "        if prev is None:\n",
    "            # training mode\n",
    "            # |x| = (batch_size, n, hidden_size)\n",
    "            z = self.masked_attn_norm(x)\n",
    "            z = x + self.masked_attn_dropout(\n",
    "                self.masked_attn(Q=z, K=z, V=z, mask=future_mask)\n",
    "            )\n",
    "        else:\n",
    "            # inference mode\n",
    "            # |x| = (batch_size, 1, hidden_size)\n",
    "            # |prev| = (batch_size, ~t-1, hidden_size)\n",
    "            normed_prev = self.masked_attn_norm(prev)\n",
    "            z = self.masked_attn_norm(x)\n",
    "            z = x + self.masked_attn_dropout(\n",
    "                self.masked_attn(Q=z, K=normed_prev, V=normed_prev, mask=None)\n",
    "            )\n",
    "\n",
    "        normed_key_and_value = self.attn_norm(key_and_value)\n",
    "        z = z + self.attn_dropout(\n",
    "            self.attn(\n",
    "                Q=self.attn_norm(z), K=normed_key_and_value, V=normed_key_and_value\n",
    "            )\n",
    "        )\n",
    "        # |z| = (batch_size, m, hidden_size)\n",
    "        z = z + self.fc_dropout(self.fc(self.fc_norm(z)))\n",
    "        # |z| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return z, key_and_value, mask, prev, future_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = CustomSequential(\n",
    "    *[DecoderBlock(hidden_size, n_splits, 0.2) for i in range(6)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, _, _, _, _ = decoder_block(emb_y, z, mask_enc, None, future_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.generator(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hidden_size, output_size)\n",
    "# generally, logit means the output before the softmax layer\n",
    "# but here names it as logit temporarily\n",
    "logit = generator(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51, 53430])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 51])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(277.8155, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit = nn.NLLLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "weight = torch.ones(output_size)\n",
    "weight[0] = 0\n",
    "crit_trial = nn.NLLLoss(weight=torch.ones(output_size), reduction=\"sum\")\n",
    "loss = crit(\n",
    "    # y_hat\n",
    "    logit.contiguous().view(-1, logit.size(-1)),\n",
    "    # y\n",
    "    y.contiguous().view(-1),\n",
    ")\n",
    "loss.div(y.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(572.0168, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = crit_trial(\n",
    "    # y_hat\n",
    "    logit.contiguous().view(-1, logit.size(-1)),\n",
    "    # y\n",
    "    y.contiguous().view(-1),\n",
    ")\n",
    "loss.div(y.size(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
