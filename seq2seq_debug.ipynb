{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chameleon.base_dataset import Vocabulary, TranslationDataset, TranslationCollator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def read_data(file_path, src_tgt):\n",
    "    data = pd.read_pickle(file_path)\n",
    "    src_lang, tgt_lang = src_tgt[:2], src_tgt[2:]\n",
    "\n",
    "    # parse source column and target column\n",
    "    src_col, tgt_col = (\"tok\" + \"_\" + src_lang, \"tok\" + \"_\" + tgt_lang)\n",
    "    srcs = data[src_col].tolist()\n",
    "    tgts = data[tgt_col].tolist()\n",
    "    return srcs, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-22 00:30:24.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[35mNone\u001b[0m | \u001b[36mchameleon.base_dataset\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mNumber of vocabularies: 30488\u001b[0m\n",
      "\u001b[32m2023-07-22 00:30:28.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[35mNone\u001b[0m | \u001b[36mchameleon.base_dataset\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mNumber of vocabularies: 53430\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_srcs, train_tgts = read_data(\"./data/chameleon.train.tok.pickle\", \"enko\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TranslationDataset(train_srcs, train_tgts, with_text=True),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    collate_fn=TranslationCollator(\n",
    "        pad_idx=Vocabulary.PAD, max_length=256, with_text=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model forward functionality\n",
    "- Data: Using first batch of the input, output data\n",
    "- `Encoder` Test\n",
    "- `Decoder` Test\n",
    "- `Attention` Test\n",
    "- `Generator` Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"input_ids\"]\n",
    "y = batch[\"output_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Note that batch has not been sorted yet.\n",
    "if isinstance(x, tuple):\n",
    "    x, x_length = x\n",
    "if isinstance(y, tuple):\n",
    "    y = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(train_loader.dataset.src_vocab)\n",
    "output_size = len(train_loader.dataset.tgt_vocab)\n",
    "word_vec_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Embedding Layer\n",
    "emb_src = nn.Embedding(input_size, word_vec_size)\n",
    "emb_tgt = nn.Embedding(output_size, word_vec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 55, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding x\n",
    "emb_x = emb_src(x)\n",
    "emb_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, num_layers, dropout):\n",
    "        # input_size – The number of expected features in the input x\n",
    "        # hidden_size – The number of features in the hidden state h\n",
    "        # num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "        # bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "        # batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
    "        # dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "        # bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "        # proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=word_vec_size,\n",
    "            hidden_size=int(hidden_size / 2),\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        if isinstance(emb, tuple):\n",
    "            x, x_lengths = emb\n",
    "            x = pack(x, x_lengths.tolist(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        y, h = self.encoder(x)\n",
    "        if isinstance(emb, tuple):\n",
    "            y, x_lengths = unpack(y, batch_first=True)\n",
    "        # |y| = (batch_size, length, hidden_size)\n",
    "\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(word_vec_size=512, hidden_size=2, num_layers=2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_src, h_0_tgt = encoder((emb_x, x_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 55, 2])\n",
      "torch.Size([4, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "# check size of h_src, h_0_tgt (hidden_state, cell_state)\n",
    "print(h_src.size())\n",
    "print(h_0_tgt[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the size of hidden, cell state derived from Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.4700e-04],\n",
       "         [-1.0582e-03],\n",
       "         [-6.1422e-04],\n",
       "         [-1.3532e-03],\n",
       "         [-1.4081e-03]],\n",
       "\n",
       "        [[ 1.5024e-09],\n",
       "         [-7.0232e-15],\n",
       "         [ 6.8798e-06],\n",
       "         [ 1.8581e-07],\n",
       "         [-5.4890e-08]],\n",
       "\n",
       "        [[-6.1933e-01],\n",
       "         [-5.5100e-01],\n",
       "         [-6.5158e-01],\n",
       "         [-5.7692e-01],\n",
       "         [-5.6602e-01]],\n",
       "\n",
       "        [[-1.3714e-01],\n",
       "         [-1.6879e-01],\n",
       "         [-1.3475e-01],\n",
       "         [-1.4646e-01],\n",
       "         [-1.5944e-01]]], grad_fn=<IndexSelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0009],\n",
       "        [-0.0011],\n",
       "        [-0.0006],\n",
       "        [-0.0014],\n",
       "        [-0.0014]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_tgt[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5024e-09],\n",
       "        [-7.0232e-15],\n",
       "        [ 6.8798e-06],\n",
       "        [ 1.8581e-07],\n",
       "        [-5.4890e-08]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_tgt[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.4700e-04,  1.5024e-09],\n",
       "        [-1.0582e-03, -7.0232e-15],\n",
       "        [-6.1422e-04,  6.8798e-06],\n",
       "        [-1.3532e-03,  1.8581e-07],\n",
       "        [-1.4081e-03, -5.4890e-08]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h_0_tgt[0][0], h_0_tgt[0][1]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |h_0_tgt| = (num_layers*2, batch_size, hidden_size/2)\n",
    "# transform the shape of tensor to (num_layers, batch_size, hidden_size)\n",
    "\n",
    "new_hiddens = []\n",
    "new_cells = []\n",
    "\n",
    "hiddens, cells = h_0_tgt\n",
    "n_layers_double = hiddens.size(0)\n",
    "\n",
    "for i in range(0, n_layers_double, 2):\n",
    "    new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)]\n",
    "    new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-9.4700e-04,  1.5024e-09],\n",
       "         [-1.0582e-03, -7.0232e-15],\n",
       "         [-6.1422e-04,  6.8798e-06],\n",
       "         [-1.3532e-03,  1.8581e-07],\n",
       "         [-1.4081e-03, -5.4890e-08]], grad_fn=<CatBackward0>),\n",
       " tensor([[-0.6193, -0.1371],\n",
       "         [-0.5510, -0.1688],\n",
       "         [-0.6516, -0.1347],\n",
       "         [-0.5769, -0.1465],\n",
       "         [-0.5660, -0.1594]], grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens = torch.stack(new_hiddens)\n",
    "new_hiddens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_encoder_hiddens(encoder_hiddens):\n",
    "    new_hiddens = []\n",
    "    new_cells = []\n",
    "\n",
    "    hiddens, cells = encoder_hiddens\n",
    "\n",
    "    # i-th and (i+1)-th layer is opposite direction.\n",
    "    # Also, each direction of layer is half hidden size.\n",
    "    # Therefore, we concatenate both directions to 1 hidden size layer.\n",
    "    for i in range(0, hiddens.size(0), 2):\n",
    "        new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)]\n",
    "        new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]\n",
    "\n",
    "    new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n",
    "    # |new_hiddens| = (n_layers, batch_size, hidden_size)\n",
    "    # |new_cells| = (n_layers, batch_size, hidden_size)\n",
    "\n",
    "    return (new_hiddens, new_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0_tgt = merge_encoder_hiddens(h_0_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_vec_size,\n",
    "        hidden_size,\n",
    "        dropout,\n",
    "        n_layers,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=word_vec_size + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
    "        batch_size = emb_t.size(0)\n",
    "        hidden_size = h_t_1[0].size(-1)\n",
    "\n",
    "        if h_t_1_tilde is None:\n",
    "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size)\n",
    "\n",
    "        # input feeding\n",
    "        x = torch.concat([emb_t, h_t_1_tilde], dim=-1)\n",
    "        y, h = self.rnn(x, h_t_1)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding tgt data\n",
    "emb_y = emb_tgt(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 55, 2]) torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(h_src.size(), h_0_tgt[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(word_vec_size=512, hidden_size=2, dropout=0.2, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 39, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_y[:, 0, :].unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1_tgt, h_c_1_tgt = decoder(emb_y[:, 0, :].unsqueeze(1), None, h_0_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 2]) torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(h_1_tgt.size(), h_c_1_tgt[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3703, -0.1409]],\n",
       "\n",
       "        [[-0.3955, -0.1939]],\n",
       "\n",
       "        [[-0.5010, -0.0566]],\n",
       "\n",
       "        [[-0.3858, -0.1498]],\n",
       "\n",
       "        [[-0.3876, -0.1760]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_1_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2283e-04,  9.3552e-01],\n",
       "         [-1.1302e-04,  7.3618e-01],\n",
       "         [-1.2514e-04,  7.4923e-01],\n",
       "         [-1.0626e-04,  7.3663e-01],\n",
       "         [-1.0453e-04,  7.3633e-01]],\n",
       "\n",
       "        [[-3.7028e-01, -1.4095e-01],\n",
       "         [-3.9554e-01, -1.9392e-01],\n",
       "         [-5.0102e-01, -5.6645e-02],\n",
       "         [-3.8585e-01, -1.4980e-01],\n",
       "         [-3.8760e-01, -1.7595e-01]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_c_1_tgt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h_src, h_t_tgt, mask=None):\n",
    "        # |h_src| = (batch_size, src_length, hidden_size)\n",
    "        # |h_t_tgt| = (batch_size, 1, hidden_size)\n",
    "        # |mask| = (batch_size, src_length)\n",
    "\n",
    "        query = self.linear(h_t_tgt)\n",
    "        # |query| = (batch_size, 1, hidden_size)\n",
    "        weight = torch.bmm(query, h_src.transpose(1, 2))\n",
    "        # |weight| = (batch_size, 1, src_length)\n",
    "\n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float(\"inf\"))\n",
    "\n",
    "        weight = self.softmax(weight)\n",
    "        context_vector = torch.bmm(weight, h_src)\n",
    "        # |context_vector| = (batch_size, 1, hidden_size)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(hidden_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 55, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_src.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(x, length):\n",
    "    mask = []\n",
    "\n",
    "    max_length = max(length)\n",
    "    for l in length:\n",
    "        if max_length - l > 0:\n",
    "            # If the length is shorter than maximum length among samples,\n",
    "            # set last few values to be 1s to remove attention weight.\n",
    "            mask += [\n",
    "                torch.cat(\n",
    "                    [x.new_ones(1, l).zero_(), x.new_ones(1, (max_length - l))], dim=-1\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            # If the length of the sample equals to maximum length among samples,\n",
    "            # set every value in mask to be 0.\n",
    "            mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "    mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "mask = generate_mask(x, x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention(h_src, h_1_tgt, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make h_t_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_linear = nn.Linear(2 * 2, 2)\n",
    "# |concat_linear| = (hidden_size*2, hidden_size)\n",
    "tanh = nn.Tanh()\n",
    "# activation layer\n",
    "\n",
    "h_t_tilde = tanh(concat_linear(torch.cat([h_1_tgt, context_vector], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, h_t_tildes):\n",
    "        # |h_t_tildes| = (batch_size, tgt_length, hidden_size)\n",
    "        y_hat = self.softmax(self.output(h_t_tildes))\n",
    "        # |y_hat| = (batch_size, tgt_length, output_size)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(2)\n",
    "\n",
    "# In real implmentation, input data should be h_t_tilde's'\n",
    "# to speed up the model forward (put all time steps at once)\n",
    "y_hat = generator(h_t_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
